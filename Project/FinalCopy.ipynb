{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe goal of this experiment is to develop a model that will predict (to a certain level of acceptable accuracy) the students final GPAs, given the current progress.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The goal of this experiment is to develop a model that will predict (to a certain level of acceptable accuracy) the students final GPAs, given the current progress.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Import helper libraries \"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Split Data into Training and Testing Sets \"\"\"\n",
    "def split_data(X, Y):\n",
    "    return train_test_split(X, Y, test_size=0.2, random_state=17)\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "def matrix_factorization(X, U, V, K, steps=5000, alpha=0.0002, beta=0.02):\n",
    "    V = V.T\n",
    "    for step in range(steps):\n",
    "        for i in range(len(X)):\n",
    "            for j in range(len(X[i])):\n",
    "                if X[i][j] > 0:\n",
    "                    eij = X[i][j] - np.dot(U[i,:],V[:,j])\n",
    "                    for k in range(K):\n",
    "                        U[i][k] = U[i][k] + alpha * (2 * eij * V[k][j] - beta * U[i][k])\n",
    "                        V[k][j] = V[k][j] + alpha * (2 * eij * U[i][k] - beta * V[k][j])\n",
    "        eX = np.dot(U, V)\n",
    "        e = 0\n",
    "        for i in range(len(X)):\n",
    "            for j in range(len(X[i])):\n",
    "                if X[i][j] > 0:\n",
    "                    e = e + pow(X[i][j] - np.dot(U[i,:],V[:,j]), 2)\n",
    "                    for k in range(K):\n",
    "                        e = e + (beta/2) * (pow(U[i][k],2) + pow(V[k][j],2))\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    \n",
    "    return U.T, V\n",
    "\n",
    "def obtain_course_cluster_matrix(X):\n",
    "    N = len(X)\n",
    "    M = len(X[0])\n",
    "\n",
    "    P = np.random.rand(N,K)\n",
    "    Q = np.random.rand(M,K)\n",
    "\n",
    "    U, V = matrix_factorization(X, P, Q, K)\n",
    "        \n",
    "    for c in range(len(courses)):\n",
    "        cluster = 0\n",
    "        for curr in range(K):\n",
    "            if V[curr][c] >= cluster:\n",
    "                cluster = curr\n",
    "        course_cluster[cluster].append(courses[c])\n",
    "        \n",
    "    return course_cluster\n",
    "\n",
    "\n",
    "def calculate_num_of_courses(semester):\n",
    "    start = -1\n",
    "    end = -1\n",
    "    if semester == 1:\n",
    "        start = 0\n",
    "        end = 5\n",
    "    elif semester == 2:\n",
    "        start = 6\n",
    "        end = 13\n",
    "    elif semester == 3:\n",
    "        start = 14\n",
    "        end = 21\n",
    "    elif semester == 4:\n",
    "        start = 22\n",
    "        end = 29\n",
    "    elif semester == 5:\n",
    "        start = 30\n",
    "        end = 37\n",
    "    elif semester == 6:\n",
    "        start = 38\n",
    "        end = 38\n",
    "    return start, end\n",
    "\n",
    "\n",
    "def cal_index_for_course(semester):\n",
    "    if semester == 1:\n",
    "        return 0\n",
    "    elif semester == 2:\n",
    "        return 6\n",
    "    elif semester == 3:\n",
    "        return 14\n",
    "    elif semester == 4:\n",
    "        return 22\n",
    "    elif semester == 5:\n",
    "        return 30\n",
    "    elif semester == 6:\n",
    "        return 38\n",
    "\n",
    "\"\"\" Extract feature (X) for target (y) column \"\"\"\n",
    "def extract_features_cols(semester, course):  \n",
    "    feature_cols = []\n",
    "    e_index = cal_index_for_course(semester)\n",
    "    \n",
    "    if(semester == 1):\n",
    "        return feature_cols\n",
    "    \n",
    "    for cluster in range(K):  # to find the cluster in which our course is.\n",
    "        if course in course_cluster[cluster]:\n",
    "            index = cluster\n",
    "            break\n",
    "    \n",
    "    my_cluster = course_cluster[index]\n",
    "    \n",
    "    for c in range(e_index):\n",
    "        if courses[c] in my_cluster:\n",
    "            feature_cols.append(courses[c])\n",
    "            \n",
    "    return feature_cols\n",
    "\n",
    "\n",
    "def random_forest_train(df, semester):\n",
    "    \"\"\"\n",
    "    To train and test accuracy using the random forest regressor with the datasets\n",
    "    :param semester:\n",
    "    :param data frame:\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n*** Random Forest: ***\\n\")\n",
    "    \n",
    "    seed = 7\n",
    "    num_trees = 100\n",
    "    s_index = cal_index_for_course(semester)\n",
    "    feature_cols = list(df.columns[:4])\n",
    "    \n",
    "    for i in range(num_of_courses[semester-1]):\n",
    "        course = courses[s_index + i]\n",
    "        feature_cols.extend(extract_features_cols(semester, course))\n",
    "        target_col = course\n",
    "        print(\"Featured_cols:  \\n\", feature_cols)\n",
    "        print(\"Target__ cols:  \\n\", target_col)\n",
    "             \n",
    "        X = df[feature_cols]  # feature values for all students\n",
    "        y = df[target_col]  # corresponding targets/labels\n",
    "        \n",
    "        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "    \n",
    "        sc_x = StandardScaler()\n",
    "        X_train = sc_x.fit_transform(X_train)\n",
    "        X_test = sc_x.transform(X_test)\n",
    "        sc_y = StandardScaler()\n",
    "        y_train = np.reshape(y_train, (-1, 1))\n",
    "        y_train = sc_y.fit_transform(y_train)\n",
    "        y_test = np.reshape(y_test, (-1, 1))\n",
    "        y_test = sc_y.transform(y_test)\n",
    "\n",
    "        regressor = RandomForestRegressor(n_estimators = num_trees, random_state = seed, oob_score = True, cv = kfold)\n",
    "        regressor.fit(X_train, y_train)\n",
    "\n",
    "        \"\"\" R^2 (coefficient of determination) regression score function \"\"\"\n",
    "        y_pred = regressor.predict(X_test)\n",
    "        print(\"r2 score: \", r2_score(y_test , y_pred))\n",
    "\n",
    "#         accuracies = cross_val_score(estimator = regressor, X = x_train, y = y_train, cv = 10)\n",
    "#         print(\"Accuracies.mean: \", accuracies.mean())\n",
    "#         print(\"S.d.: \", accuracies.std())\n",
    "        \"\"\" Mean squared error regression loss \"\"\"\n",
    "        mse_test = mean_squared_error(y_test, y_pred)\n",
    "        print(\"Mean squared error: \", mse_test)\n",
    "        \n",
    "#         \"\"\"  Mean squared logarithmic regression loss \"\"\"\n",
    "#         mse_test = mean_squared_log_error(y_test, y_pred)\n",
    "#         print(\"Mean squared logarithmic error: \", mse_test)\n",
    "        \n",
    "       \n",
    "        # instantiate a random forest model\n",
    "#         model = RandomForestRegressor(n_estimators=num_trees)\n",
    "#         results = model_selection.cross_val_score(model, X, y, cv=kfold)\n",
    "#         # check the accuracy on the training set\n",
    "#         print(\"accuracy:\", results.mean())\n",
    "        \n",
    "        feature_cols = list(df.columns[:4])\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "\"\"\" kNN Algorithm for training the model using datasets \"\"\"\n",
    "def knn_train(df, semester):\n",
    "    \"\"\"\n",
    "    To train the model using the kNN algorithm with the datasets\n",
    "    :param data frame:\n",
    "    :param semester:\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\\n*** KNN ***:\\n\")\n",
    "    s_index = cal_index_for_course(semester)\n",
    "    feature_cols = list(df.columns[:4])  #initialise with static features\n",
    "    \n",
    "    for i in range(num_of_courses[semester-1]):\n",
    "        course = courses[s_index + i]\n",
    "        feature_cols.extend(extract_features_cols(semester, course))\n",
    "        target_col = course\n",
    "        print(\"Featured_cols:  \\n\", feature_cols)\n",
    "        print(\"Target__ cols:  \\n\", target_col)\n",
    "        \n",
    "        \n",
    "        X = df[feature_cols]  # feature values for all students\n",
    "        y = df[target_col]  # corresponding targets/labels\n",
    "          \n",
    "        X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "    \n",
    "        # instantiate a kNN regression model, and fit with X and y\n",
    "        model = neighbors.KNeighborsRegressor()\n",
    "        model = model.fit(X_train, y_train)\n",
    "        \n",
    "        sc_x = StandardScaler()\n",
    "        X_train = sc_x.fit_transform(X_train)\n",
    "        X_test = sc_x.transform(X_test)\n",
    "        sc_y = StandardScaler()\n",
    "        y_train = np.reshape(y_train, (-1, 1))\n",
    "        y_train = sc_y.fit_transform(y_train)\n",
    "        y_test = np.reshape(y_test, (-1, 1))\n",
    "        y_test = sc_y.transform(y_test)\n",
    "\n",
    "        regressor = neighbors.KNeighborsRegressor()\n",
    "        regressor = regressor.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "        \"\"\" R^2 (coefficient of determination) regression score function \"\"\"\n",
    "        y_pred = regressor.predict(X_test)\n",
    "        print(\"r2 score: \", r2_score(y_test , y_pred))\n",
    "\n",
    "        \"\"\" Mean squared error regression loss \"\"\"\n",
    "        mse_test = mean_squared_error(y_test, y_pred)\n",
    "        print(\"Mean squared error: \", mse_test)\n",
    "        \n",
    "#         \"\"\"  Mean squared logarithmic regression loss \"\"\"\n",
    "#         mse_test = mean_squared_log_error(y_test, y_pred)\n",
    "#         print(\"Mean squared logarithmic error: \", mse_test)\n",
    "        \n",
    "        feature_cols = list(df.columns[:4])\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "\"\"\" SVM Algorithm for training the model using datasets \"\"\"\n",
    "def svm_train(df, semester):\n",
    "    \"\"\"\n",
    "    To train the model using the kNN algorithm with the datasets\n",
    "    :param data frame:\n",
    "    :param semester:\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\\n*** SVM ***:\\n\")\n",
    "    s_index = cal_index_for_course(semester)\n",
    "    feature_cols = list(df.columns[:4])  #initialise with static features\n",
    "    for i in range(num_of_courses[semester-1]):\n",
    "        course = courses[s_index + i]\n",
    "        feature_cols.extend(extract_features_cols(semester, course))\n",
    "        target_col = course\n",
    "        print(\"Featured_cols:  \\n\", feature_cols)\n",
    "        print(\"Target__ cols:  \\n\", target_col)\n",
    "        \n",
    "        \n",
    "        X = df[feature_cols]  # feature values for all students\n",
    "        y = df[target_col]  # corresponding targets/labels\n",
    "          \n",
    "        X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "    \n",
    "        # instantiate a kNN regression model, and fit with X and y\n",
    "        model = neighbors.KNeighborsRegressor()\n",
    "        model = model.fit(X_train, y_train)\n",
    "        \n",
    "        sc_x = StandardScaler()\n",
    "        X_train = sc_x.fit_transform(X_train)\n",
    "        X_test = sc_x.transform(X_test)\n",
    "        sc_y = StandardScaler()       \n",
    "        y_train = np.reshape(y_train, (-1, 1))\n",
    "        y_train = sc_y.fit_transform(y_train)\n",
    "        y_test = np.reshape(y_test, (-1, 1))\n",
    "        y_test = sc_y.transform(y_test)\n",
    "\n",
    "        regressor = svm.SVR(C=1.0, epsilon=0.2)\n",
    "        regressor = regressor.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "        \"\"\" R^2 (coefficient of determination) regression score function \"\"\"\n",
    "        y_pred = regressor.predict(X_test)\n",
    "        print(\"r2 score: \", r2_score(y_test , y_pred))\n",
    "\n",
    "        \"\"\" Mean squared error regression loss \"\"\"\n",
    "        mse_test = mean_squared_error(y_test, y_pred)\n",
    "        print(\"Mean squared error: \", mse_test)\n",
    "        \n",
    "#         \"\"\"  Mean squared logarithmic regression loss \"\"\"\n",
    "#         mse_test = mean_squared_log_error(y_test, y_pred)\n",
    "#         print(\"Mean squared logarithmic error: \", mse_test)\n",
    "        \n",
    "        feature_cols = list(df.columns[:4])\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        feature_cols = list(df.columns[:4])        \n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "\n",
    "def linear_regression_train(semester, df):\n",
    "    \"\"\"\n",
    "    To train and test accuracy using the linear regression with the datasets\n",
    "    :param semester:\n",
    "    :param data frame:\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\\n*** Linear Regression: ***\\n\")\n",
    "    \n",
    "    s_index = cal_index_for_course(semester)\n",
    "    feature_cols = list(df.columns[:4])\n",
    "    \n",
    "    for i in range(num_of_courses[semester-1]):\n",
    "        course = courses[s_index + i]\n",
    "        feature_cols.extend(extract_features_cols(semester, course))\n",
    "        target_col = course\n",
    "        print(\"Featured_cols:  \\n\", feature_cols)\n",
    "        print(\"Target__ cols:  \\n\", target_col)\n",
    "        \n",
    "        \n",
    "        X = df[feature_cols]  # feature values for all students\n",
    "        y = df[target_col]  # corresponding targets/labels\n",
    "          \n",
    "        X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "    \n",
    "        # instantiate a kNN regression model, and fit with X and y\n",
    "        model = neighbors.KNeighborsRegressor()\n",
    "        model = model.fit(X_train, y_train)\n",
    "\n",
    "        print(model.predict(X_test))\n",
    "        \n",
    "        sc_x = StandardScaler()\n",
    "        X_train = sc_x.fit_transform(X_train)\n",
    "        X_test = sc_x.transform(X_test)\n",
    "        sc_y = StandardScaler()\n",
    "        y_train = np.reshape(y_train, (-1, 1))\n",
    "        y_train = sc_y.fit_transform(y_train)\n",
    "        y_test = np.reshape(y_test, (-1, 1))\n",
    "        y_test = sc_y.transform(y_test)\n",
    "\n",
    "        regressor = LinearRegression()\n",
    "        regressor = regressor.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "        \"\"\" R^2 (coefficient of determination) regression score function \"\"\"\n",
    "        y_pred = regressor.predict(X_test)\n",
    "        print(\"r2 score: \", r2_score(y_test , y_pred))\n",
    "\n",
    "        \"\"\" Mean squared error regression loss \"\"\"\n",
    "        mse_test = mean_squared_error(y_test, y_pred)\n",
    "        print(\"Mean squared error: \", mse_test)\n",
    "        \n",
    "#         \"\"\"  Mean squared logarithmic regression loss \"\"\"\n",
    "#         mse_test = mean_squared_log_error(y_test, y_pred)\n",
    "#         print(\"Mean squared logarithmic error: \", mse_test)\n",
    "        \n",
    "        feature_cols = list(df.columns[:4])\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        feature_cols = list(df.columns[:4])        \n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    \"\"\" Read data file as DataFrame \"\"\"\n",
    "    df = pd.read_csv(\"dataset.txt\", sep=\",\")\n",
    "    \n",
    "    courses_df = df.loc[:, df.columns.isin(list(courses))]\n",
    "    \n",
    "    X = courses_df.as_matrix()\n",
    "    \n",
    "    semester = K\n",
    "    \n",
    "    course_cluster = obtain_course_cluster_matrix(X)\n",
    "    \n",
    "    print(\"Course Cluster:\\n\")    \n",
    "    for item in course_cluster:\n",
    "        print(item)\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    start, end = calculate_num_of_courses(semester)\n",
    "    \n",
    "    svm_train(df, semester)    \n",
    "    knn_train(df, semester)\n",
    "    linear_regression_train(df, semester)\n",
    "    random_forest_train(df, semester)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semester: 6\n",
      "Course Cluster:\n",
      "\n",
      "[]\n",
      "[]\n",
      "['psqt_gpa', 'co_gpa', 'pscp_gpa', 'foss_lab_gpa', 'afm_gpa', 'ds_gpa', 'ss_gpa', 'ds_lab_gpa', 'ss_lab_gpa', 'os_gpa', 'dbms_gpa', 'ads_gpa', 'os_lab_gpa', 'dbms_lab_gpa', 'pdwdm_gpa', 'ccn_gpa', 'dos_gpa', 'ke_lab_gpa', 'ccn_lab_gpa', 'nps_gpa', 'nps_lab_gpa']\n",
      "['me_gpa', 'pscp_lab_gpa', 'oops_gpa', 'aad_gpa', 'oops_lab_gpa', 'se_gpa', 'ooad_gpa', 'set_lab_gpa', 'uc_gpa', 'cc_gpa', 'dpsa_gpa', 'stt_gpa', 'seminar_viva_gpa', 'project_gpa']\n",
      "['ospm_gpa', 'stt_lab_gpa']\n",
      "['wt_gpa', 'wt_lab_gpa']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*** SVM ***:\n",
      "\n",
      "Featured_cols:  \n",
      " ['ten_gpa', 'twelth_gpa', 'colz_gpa', 'nimcet_marks', 'me_gpa', 'pscp_lab_gpa', 'oops_gpa', 'aad_gpa', 'oops_lab_gpa', 'se_gpa', 'ooad_gpa', 'set_lab_gpa', 'uc_gpa', 'cc_gpa', 'dpsa_gpa', 'stt_gpa', 'seminar_viva_gpa']\n",
      "Target__ cols:  \n",
      " project_gpa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KRISHNA\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:52: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n",
      "C:\\Users\\KRISHNA\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\KRISHNA\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score:  0.43497136047469964\n",
      "Mean squared error:  0.40287756345419645\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*** KNN ***:\n",
      "\n",
      "Featured_cols:  \n",
      " ['ten_gpa', 'twelth_gpa', 'colz_gpa', 'nimcet_marks', 'me_gpa', 'pscp_lab_gpa', 'oops_gpa', 'aad_gpa', 'oops_lab_gpa', 'se_gpa', 'ooad_gpa', 'set_lab_gpa', 'uc_gpa', 'cc_gpa', 'dpsa_gpa', 'stt_gpa', 'seminar_viva_gpa']\n",
      "Target__ cols:  \n",
      " project_gpa\n",
      "r2 score:  0.150354609929078\n",
      "Mean squared error:  0.6058154235145385\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*** Linear Regression: ***\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f819f78a7fc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mcourse_cluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-29e48c86d920>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0msvm_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msemester\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mknn_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msemester\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mlinear_regression_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msemester\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mrandom_forest_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msemester\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-437bbab048e3>\u001b[0m in \u001b[0;36mlinear_regression_train\u001b[1;34m(semester, df)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\n*** Linear Regression: ***\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m     \u001b[0ms_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcal_index_for_course\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msemester\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m     \u001b[0mfeature_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-437bbab048e3>\u001b[0m in \u001b[0;36mcal_index_for_course\u001b[1;34m(semester)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcal_index_for_course\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msemester\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0msemester\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0msemester\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[0;32m   1120\u001b[0m                          \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# List of all courses offered in MCA program \n",
    "courses = ['psqt_gpa', 'me_gpa', 'co_gpa', 'pscp_gpa', 'pscp_lab_gpa', 'foss_lab_gpa', \n",
    "           'afm_gpa', 'ds_gpa', 'ss_gpa', 'oops_gpa', 'aad_gpa', 'ds_lab_gpa', 'ss_lab_gpa', 'oops_lab_gpa', \n",
    "           'os_gpa', 'dbms_gpa', 'wt_gpa', 'ospm_gpa', 'ads_gpa', 'os_lab_gpa', 'dbms_lab_gpa', 'wt_lab_gpa', \n",
    "           'pdwdm_gpa', 'ccn_gpa', 'se_gpa', 'ooad_gpa', 'dos_gpa', 'ke_lab_gpa', 'ccn_lab_gpa', 'set_lab_gpa', \n",
    "           'nps_gpa', 'uc_gpa', 'cc_gpa', 'dpsa_gpa', 'stt_gpa', 'seminar_viva_gpa', 'nps_lab_gpa', 'stt_lab_gpa', \n",
    "           'project_gpa']\n",
    "\n",
    "num_of_courses = [6, 8, 8, 8, 8, 1] #number of courses per semester\n",
    " \n",
    "K = int(input(\"Semester: \")) # total number of clusters\n",
    "\n",
    "# relevant(or dependent) courses are grouped in same cluster\n",
    "course_cluster=[]\n",
    "for k in range(K):\n",
    "    course_cluster.append([])\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
